{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d3dd957d",
      "metadata": {
        "id": "d3dd957d",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "source": [
        "# Inference Optimizations\n",
        "Note: used google collab for GPU because of time constraint. \\\n",
        "In real pipeline this would be done after training on GPU instance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kj8Izgcb689h",
      "metadata": {
        "id": "kj8Izgcb689h"
      },
      "source": [
        "## Pre-process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2YjN67g88YE-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "2YjN67g88YE-",
        "outputId": "01b76ebc-047c-450e-975c-ed0eccba67f1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-039e1dc7-020f-42a7-b0d9-a38957f59c1c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-039e1dc7-020f-42a7-b0d9-a38957f59c1c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "ref                                                          title                                                      size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "-----------------------------------------------------------  --------------------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "jayaantanaath/student-habits-vs-academic-performance         Student Habits vs Academic Performance                    19512  2025-04-12 10:49:08.663000          24300        415  1.0              \n",
            "adilshamim8/cost-of-international-education                  Cost of International Education                           18950  2025-05-07 15:41:53.213000           5202         87  1.0              \n",
            "adilshamim8/social-media-addiction-vs-relationships          Students' Social Media Addiction                           7851  2025-05-10 14:38:02.713000           2519         39  1.0              \n",
            "ivankmk/thousand-ml-jobs-in-usa                              Machine Learning Job Postings in the US                 1682058  2025-04-20 16:11:59.347000           4919        108  1.0              \n",
            "fatemehmohammadinia/heart-attack-dataset-tarik-a-rashid      Heart Attack Dataset                                      16250  2025-04-30 21:58:22.740000           4817         80  1.0              \n",
            "mahdimashayekhi/fake-news-detection-dataset                  Fake News Detection Dataset                            11735585  2025-04-27 14:52:10.607000           2075         28  1.0              \n",
            "michaelmatta0/global-development-indicators-2000-2020        Global Development Full Analysis (2000-2020)            1311638  2025-05-11 16:57:19.013000            842         26  1.0              \n",
            "madhuraatmarambhagat/crop-recommendation-dataset             Crop Recommendation Dataset                               65234  2025-05-08 17:02:09.397000            918         29  1.0              \n",
            "aryan208/financial-transactions-dataset-for-fraud-detection  Financial Transactions Dataset for Fraud Detection    290256858  2025-05-02 09:12:28.203000           1296         30  1.0              \n",
            "umeradnaan/daily-social-media-active-users                   Daily Social Media Active Users                          126814  2025-05-05 02:11:50.873000           1747         23  1.0              \n",
            "khushikyad001/impact-of-screen-time-on-mental-health         Impact of Screen Time on Mental Health                    64873  2025-04-20 18:01:47.570000           2854         43  1.0              \n",
            "zahidmughal2343/global-cancer-patients-2015-2024             global_cancer_patients_2015_2024                        1261049  2025-04-14 00:05:23.367000           5348         65  1.0              \n",
            "razanaqvi14/real-and-fake-news                               Real & Fake News                                       42975911  2025-04-28 19:46:53.073000           1133         24  1.0              \n",
            "wikimedia-foundation/wikipedia-structured-contents           Wikipedia Structured Contents                       25121685657  2025-04-11 07:11:03.397000           2402        298  0.8125           \n",
            "adilshamim8/greenhouse-plant-growth-metrics                  Greenhouse Plant Growth                                 3041046  2025-04-19 07:33:57.787000           1833         28  1.0              \n",
            "stephennanga/malawi-datasets                                  Malawi National Football Team Matches                     1306  2025-04-07 23:07:51.297000           1404         43  1.0              \n",
            "nikolasgegenava/sneakers-classification                      Popular Sneakers Classification                        17981294  2025-05-01 12:00:45.517000           1677         46  1.0              \n",
            "dnkumars/cryptocurrency-transaction-analytics-btc-and-eth    Cryptocurrency Transaction Analytics: BTC & ETH         5167978  2025-05-11 15:16:52.107000            441         38  1.0              \n",
            "palvinder2006/ola-bike-ride-request                          Ola Bike Ride Request                                    174975  2025-04-28 03:55:33.860000           1123         28  1.0              \n",
            "brendanartley/openfwi-preprocessed-72x72                     OpenFWI Preprocessed 72x72                          21254845946  2025-05-13 22:11:02.327000            501         18  1.0              \n"
          ]
        }
      ],
      "source": [
        "# Upload kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# Move to the correct path\n",
        "!mkdir -p /content/.kaggle\n",
        "!cp kaggle.json /content/.kaggle/\n",
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "\n",
        "# Set the environment variable so the API knows where to look\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/.kaggle\"\n",
        "\n",
        "# Test\n",
        "!kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "brjGUd9A74BN",
      "metadata": {
        "id": "brjGUd9A74BN"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import zipfile\n",
        "\n",
        "# os.environ[\"KAGGLE_CONFIG_DIR\"] = os.path.abspath(\".kaggle\") # Use local .kaggle directory\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "wlF8J4Je72OJ",
      "metadata": {
        "id": "wlF8J4Je72OJ"
      },
      "outputs": [],
      "source": [
        "def download_jigsaw(kaggle_dir):\n",
        "    os.makedirs(kaggle_dir, exist_ok=True)\n",
        "\n",
        "    api = KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "    # Download competition data\n",
        "    api.competition_download_files(\n",
        "        \"jigsaw-unintended-bias-in-toxicity-classification\",\n",
        "        path=kaggle_dir\n",
        "    )\n",
        "\n",
        "    # Unzip\n",
        "    zip_path = os.path.join(kaggle_dir, \"jigsaw-unintended-bias-in-toxicity-classification.zip\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(kaggle_dir)\n",
        "\n",
        "    print(\"Downloaded and extracted Jigsaw dataset.\")\n",
        "\n",
        "\n",
        "def preprocess(kaggle_dir, output_dir, split_ratio=0.2):\n",
        "    # Create output directory if missing\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Ensure input file exists, create parent dir if needed (just in case)\n",
        "    os.makedirs(kaggle_dir, exist_ok=True)\n",
        "    input_path = os.path.join(kaggle_dir, \"train.csv\")\n",
        "    if not os.path.exists(input_path):\n",
        "        raise FileNotFoundError(f\"train.csv not found in {kaggle_dir}\")\n",
        "\n",
        "    df = pd.read_csv(input_path).dropna(subset=[\"comment_text\"])\n",
        "\n",
        "    # Keep only the needed columns\n",
        "    df = df[[\"comment_text\", \"target\"]]\n",
        "\n",
        "    # Binarize target (optional: uncomment if needed)\n",
        "    # df[\"target\"] = (df[\"target\"] >= 0.5).astype(int)\n",
        "\n",
        "    # Split\n",
        "    train_df, val_df = train_test_split(df, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    train_df.to_csv(os.path.join(output_dir, \"train.csv\"), index=False)\n",
        "    val_df.to_csv(os.path.join(output_dir, \"val.csv\"), index=False)\n",
        "\n",
        "    print(f\"Saved {len(train_df)} training and {len(val_df)} validation samples to {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "s83yLZkY784G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s83yLZkY784G",
        "outputId": "6c585ec8-f509-4c0d-850b-7ea2d48288a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded and extracted Jigsaw dataset.\n",
            "Saved 1443896 training and 360975 validation samples to data/jigsaw/processed/\n"
          ]
        }
      ],
      "source": [
        "kaggle_dir = \"data/jigsaw/raw/\"\n",
        "output_dir = \"data/jigsaw/processed/\"\n",
        "val_split = 0.2\n",
        "\n",
        "download_jigsaw(kaggle_dir)\n",
        "preprocess(kaggle_dir, output_dir, val_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W6WLW_2G7z2B",
      "metadata": {
        "id": "W6WLW_2G7z2B"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "zGBzK-M26-VF",
      "metadata": {
        "id": "zGBzK-M26-VF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "68nz89tK6Kmo",
      "metadata": {
        "id": "68nz89tK6Kmo"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"initial_epochs\": 2,\n",
        "    \"total_epochs\": 1,\n",
        "    \"patience\": 2,\n",
        "    \"batch_size\": 128,\n",
        "    \"lr\": 2e-5,\n",
        "    \"fine_tune_lr\": 1e-5,\n",
        "    \"max_len\": 128,\n",
        "    \"dropout_probability\": 0.3,\n",
        "    \"model_name\": \"google/bert_uncased_L-2_H-128_A-2\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "KSfh49tW6unS",
      "metadata": {
        "id": "KSfh49tW6unS"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Dataset\n",
        "# ---------------------------\n",
        "class JigsawDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.texts = df[\"comment_text\"].tolist()\n",
        "        self.labels = (df[\"target\"] >= 0.5).astype(int).tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "QJrmvVAy6ys6",
      "metadata": {
        "id": "QJrmvVAy6ys6"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Training + Evaluation Functions\n",
        "# ---------------------------\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device, portion=0.01):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    num_batches = int(portion * len(loader)) # Doing part of the training because my part is inference and monitoring\n",
        "    print(f\"Training for {num_batches} batches\")\n",
        "\n",
        "    for i, batch in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
        "        if i >= num_batches:\n",
        "            break\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = criterion(outputs.logits.view(-1), batch[\"labels\"].float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        probs = torch.sigmoid(outputs.logits.view(-1))\n",
        "        preds = (probs > 0.5).long()\n",
        "        correct += (preds == batch[\"labels\"]).sum().item()\n",
        "        total += batch[\"labels\"].size(0)\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_acc = correct / total\n",
        "    print(f\"Partial Epoch Summary - Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_acc:.4f}\\n\")\n",
        "\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "def evaluate(model, loader, criterion, device, portion=0.01):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    num_batches = int(portion * len(loader))\n",
        "    print(f\"Evaluating for {num_batches} batches\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(loader, desc=\"Evaluating\", leave=False)):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = criterion(outputs.logits.view(-1), batch[\"labels\"].float())\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            probs = torch.sigmoid(outputs.logits.view(-1))\n",
        "            preds = (probs > 0.5).long()\n",
        "            correct += (preds == batch[\"labels\"]).sum().item()\n",
        "            total += batch[\"labels\"].size(0)\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_acc = correct / total\n",
        "    print(f\"Eval Summary - Avg Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\\n\")\n",
        "\n",
        "    return avg_loss, avg_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "MWPCEBp17Yk0",
      "metadata": {
        "id": "MWPCEBp17Yk0"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Main Training Pipeline\n",
        "# ---------------------------\n",
        "def main(args):\n",
        "    # made to run in command line originally\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument(\"--data-dir\", type=str, required=True, help=\"Directory with train.csv and val.csv\")\n",
        "    # parser.add_argument(\"--save-path\", type=str, required=True, help=\"Path to save the trained model\")\n",
        "    # parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"Run a quick test on a small sample\")\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    os.makedirs(os.path.dirname(args.save_path), exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
        "    train_df = pd.read_csv(os.path.join(args.data_dir, \"train.csv\"))\n",
        "    val_df = pd.read_csv(os.path.join(args.data_dir, \"val.csv\"))\n",
        "\n",
        "    if args.dry_run:\n",
        "        train_df = train_df.sample(n=32, random_state=42)\n",
        "        val_df = val_df.sample(n=32, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(JigsawDataset(train_df, tokenizer, config[\"max_len\"]), batch_size=config[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(JigsawDataset(val_df, tokenizer, config[\"max_len\"]), batch_size=config[\"batch_size\"], num_workers=8, pin_memory=True)\n",
        "\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "      config[\"model_name\"],\n",
        "      num_labels=1  # binary classification\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience_counter = 0\n",
        "    batch_portion = 1.0\n",
        "\n",
        "    for epoch in range(config[\"total_epochs\"]):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, portion=batch_portion)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device, portion=batch_portion)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} Acc={train_acc:.4f} | Val Loss={val_loss:.4f} Acc={val_acc:.4f} | Time={time.time() - start:.2f}s\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), args.save_path)\n",
        "            patience_counter = 0\n",
        "            print(\"  Validation loss improved. Model saved.\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  No improvement. Patience: {patience_counter}\")\n",
        "            if patience_counter >= config[\"patience\"]:\n",
        "                print(\"  Early stopping.\")\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "v1sjxCQx-edf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1sjxCQx-edf",
        "outputId": "37052ad3-f0b0-4af5-b91f-110458eadcee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 11281 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partial Epoch Summary - Avg Loss: 0.1735, Avg Accuracy: 0.9397\n",
            "\n",
            "Evaluating for 2821 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Summary - Avg Loss: 0.1372, Accuracy: 0.9476\n",
            "\n",
            "Epoch 1: Train Loss=0.1735 Acc=0.9397 | Val Loss=0.1372 Acc=0.9476 | Time=217.46s\n",
            "  Validation loss improved. Model saved.\n"
          ]
        }
      ],
      "source": [
        "# simulate arguments\n",
        "class args:\n",
        "  data_dir = \"data/jigsaw/processed/\"\n",
        "  save_path = \"models/model.pth\"\n",
        "  dry_run = False\n",
        "\n",
        "main(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k8e-A7-c9oC_",
      "metadata": {
        "id": "k8e-A7-c9oC_"
      },
      "source": [
        "## Inference Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "qkzLnieUCItb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkzLnieUCItb",
        "outputId": "eb07d9b5-64aa-4d1a-ecca-58e6def0c1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "JcGYc7QZ9o4F",
      "metadata": {
        "id": "JcGYc7QZ9o4F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from torchinfo import summary\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "NwXGzxLoHIss",
      "metadata": {
        "id": "NwXGzxLoHIss"
      },
      "outputs": [],
      "source": [
        "class JigsawDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.texts = df[\"comment_text\"].tolist()\n",
        "        self.labels = (df[\"target\"] >= 0.5).astype(int).tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "fUFo_RINHMzx",
      "metadata": {
        "id": "fUFo_RINHMzx"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "max_len = 128\n",
        "model_name = \"google/bert_uncased_L-2_H-128_A-2\"\n",
        "dataset_dir = os.getenv(\"DATA_DIR\", \"data/jigsaw/processed\")\n",
        "model_path = \"models/model.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "WBvbzUaXHNaU",
      "metadata": {
        "id": "WBvbzUaXHNaU"
      },
      "outputs": [],
      "source": [
        "val_df = pd.read_csv(os.path.join(dataset_dir, \"val.csv\"))\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "test_loader = DataLoader(JigsawDataset(val_df, tokenizer, max_len), batch_size=batch_size, shuffle=False, num_workers=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D_94H62kHLOu",
      "metadata": {
        "id": "D_94H62kHLOu"
      },
      "source": [
        "### Measure inference performance of PyTorch model on CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "twIapCTmvbVJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twIapCTmvbVJ",
        "outputId": "74908a9d-2a2d-40fb-b252-71a466d2267c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=====================================================================================\n",
              "Layer (type:depth-idx)                                       Param #\n",
              "=====================================================================================\n",
              "BertForSequenceClassification                                --\n",
              "├─BertModel: 1-1                                             --\n",
              "│    └─BertEmbeddings: 2-1                                   --\n",
              "│    │    └─Embedding: 3-1                                   3,906,816\n",
              "│    │    └─Embedding: 3-2                                   65,536\n",
              "│    │    └─Embedding: 3-3                                   256\n",
              "│    │    └─LayerNorm: 3-4                                   256\n",
              "│    │    └─Dropout: 3-5                                     --\n",
              "│    └─BertEncoder: 2-2                                      --\n",
              "│    │    └─ModuleList: 3-6                                  396,544\n",
              "│    └─BertPooler: 2-3                                       --\n",
              "│    │    └─Linear: 3-7                                      16,512\n",
              "│    │    └─Tanh: 3-8                                        --\n",
              "├─Dropout: 1-2                                               --\n",
              "├─Linear: 1-3                                                129\n",
              "=====================================================================================\n",
              "Total params: 4,386,049\n",
              "Trainable params: 4,386,049\n",
              "Non-trainable params: 0\n",
              "====================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "state_dict = torch.load(model_path, map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.compile() # Test Compile mode\n",
        "model.eval()\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "ts-e_SnJvlqB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts-e_SnJvlqB",
        "outputId": "22c2d0e8-62bf-46bb-8121-56614b941078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size on Disk: 17.56 MB\n"
          ]
        }
      ],
      "source": [
        "model_size = os.path.getsize(model_path)\n",
        "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "jtAMTLh2wGYK",
      "metadata": {
        "id": "jtAMTLh2wGYK"
      },
      "outputs": [],
      "source": [
        "def evaluate_test(model, loader, device=None, portion=0.01):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device) # test accuracy of gpu if no device specified\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    num_batches = int(portion * len(loader))\n",
        "    print(f\"Evaluating for {num_batches} batches on device: {device}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(loader, desc=\"Evaluating\", leave=False)):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            preds = outputs.logits.argmax(dim=1)\n",
        "            correct += (preds == batch[\"labels\"]).sum().item()\n",
        "            total += batch[\"labels\"].size(0)\n",
        "\n",
        "    return correct, total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "vxzuJtb3voNE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxzuJtb3voNE",
        "outputId": "245faf81-d577-4407-c702-e73b1826c037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating for 2821 batches on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   0%|          | 0/2821 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
            "  warnings.warn(\n",
            "                                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 92.02% (332165/360975 correct)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "correct, total = evaluate_test(model, test_loader, portion=1.0)\n",
        "accuracy = (correct / total) * 100\n",
        "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8hqE-v5Nwpq1",
      "metadata": {
        "id": "8hqE-v5Nwpq1"
      },
      "source": [
        "#### Inference Latency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "LCGGR__Yvqqj",
      "metadata": {
        "id": "LCGGR__Yvqqj"
      },
      "outputs": [],
      "source": [
        "num_trials = 100\n",
        "\n",
        "# 1) get one batch as a dict\n",
        "batch = next(iter(test_loader))\n",
        "# 2) extract the first example and move to device\n",
        "input_ids      = batch[\"input_ids\"][0].unsqueeze(0).to(device)\n",
        "attention_mask = batch[\"attention_mask\"][0].unsqueeze(0).to(device)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 3) warm-up\n",
        "with torch.no_grad():\n",
        "    _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# 4) timed runs\n",
        "latencies = []\n",
        "for _ in range(num_trials):\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    latencies.append(time.perf_counter() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "HG7xz3erwt3v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG7xz3erwt3v",
        "outputId": "8ee5c641-f2aa-4f8c-8a06-a9634ad53338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Latency (single sample, median): 1.82 ms\n",
            "Inference Latency (single sample, 95th percentile): 2.02 ms\n",
            "Inference Latency (single sample, 99th percentile): 2.28 ms\n",
            "Inference Throughput (single sample): 540.00 FPS\n"
          ]
        }
      ],
      "source": [
        "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
        "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
        "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
        "print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D6AS3pLTw1HL",
      "metadata": {
        "id": "D6AS3pLTw1HL"
      },
      "source": [
        "#### Batch throughput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "dgYn-pGQw1ra",
      "metadata": {
        "id": "dgYn-pGQw1ra"
      },
      "outputs": [],
      "source": [
        "num_batches = 10  # Number of trials\n",
        "\n",
        "# 1) Grab one batch (a dict) and move to device, dropping labels\n",
        "batch = next(iter(test_loader))\n",
        "batch = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 2) Warm-up\n",
        "with torch.no_grad():\n",
        "    model(**batch)\n",
        "\n",
        "# 3) Timed runs\n",
        "batch_times = []\n",
        "for _ in range(num_batches):\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        model(**batch)\n",
        "    batch_times.append(time.perf_counter() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "HgmK33Nxw5Kv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgmK33Nxw5Kv",
        "outputId": "f8b05afa-0773-4ee5-e0ee-d4e98adf7c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Throughput: 2170.57 FPS\n"
          ]
        }
      ],
      "source": [
        "# assume `batch` is the dict you moved to device and `batch_times` is your list of durations\n",
        "batch_size    = batch[\"input_ids\"].shape[0]\n",
        "total_samples = batch_size * num_batches\n",
        "batch_fps     = total_samples / np.sum(batch_times)\n",
        "\n",
        "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VRN1vHcWw_wK",
      "metadata": {
        "id": "VRN1vHcWw_wK"
      },
      "source": [
        "#### Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "9hpuiP8Ew-QV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hpuiP8Ew-QV",
        "outputId": "6b79bed5-78dc-4a55-b087-40df1237be74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size on Disk: 17.56 MB\n",
            "Accuracy: 92.02% (332165/360975 correct)\n",
            "Inference Latency (single sample, median): 1.82 ms\n",
            "Inference Latency (single sample, 95th percentile): 2.02 ms\n",
            "Inference Latency (single sample, 99th percentile): 2.28 ms\n",
            "Inference Throughput (single sample): 540.00 FPS\n",
            "Batch Throughput: 2170.57 FPS\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")\n",
        "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
        "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
        "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
        "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
        "print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n",
        "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v0a1JSqjxHoC",
      "metadata": {
        "id": "v0a1JSqjxHoC"
      },
      "source": [
        "#### **Eager mode Summary**\n",
        "Model Size on Disk: 17.56 MB \\\n",
        "Accuracy: 92.02% (332165/360975 correct) \\\n",
        "Inference Latency (single sample, median): 3.61 ms \\\n",
        "Inference Latency (single sample, 95th percentile): 4.03 ms \\\n",
        "Inference Latency (single sample, 99th percentile): 4.76 ms \\\n",
        "Inference Throughput (single sample): 270.66 FPS \\\n",
        "Batch Throughput: 1358.45 FPS \\\n",
        "\n",
        "#### **Compiled Summary**\n",
        "Model Size on Disk: 17.56 MB \\\n",
        "Accuracy: 92.02% (332165/360975 correct) \\\n",
        "Inference Latency (single sample, median): 1.82 ms \\\n",
        "Inference Latency (single sample, 95th percentile): 2.02 ms \\\n",
        "Inference Latency (single sample, 99th percentile): 2.28 ms \\\n",
        "Inference Throughput (single sample): 540.00 FPS \\\n",
        "Batch Throughput: 2170.57 FPS \\"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WlQQOubWCD-Q",
      "metadata": {
        "id": "WlQQOubWCD-Q"
      },
      "source": [
        "### Measure inference performance of ONNX model on CPU¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "Yxl8fQkUG3K7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxl8fQkUG3K7",
        "outputId": "e863e569-eed0-405f-8752-92a84f4a178e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.13.2)\n",
            "Collecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime_gpu-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (283.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.18.0 onnxruntime-gpu-1.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx onnxruntime-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "B9a2EukdCHQS",
      "metadata": {
        "id": "B9a2EukdCHQS"
      },
      "outputs": [],
      "source": [
        "import onnx\n",
        "import onnxruntime as ort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "fQ3JXMp4HAIt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ3JXMp4HAIt",
        "outputId": "0d70df51-cbe5-438e-f544-46db41c67ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "state_dict = torch.load(model_path, map_location=device)\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "KBpJ0ljSHY0V",
      "metadata": {
        "id": "KBpJ0ljSHY0V"
      },
      "outputs": [],
      "source": [
        "onnx_model_path = \"models/model.onnx\"\n",
        "\n",
        "# dummy input - used to clarify the input shape\n",
        "batch_size = 1\n",
        "seq_len    = max_len\n",
        "dummy_input_ids = torch.randint(\n",
        "    low=0,\n",
        "    high=tokenizer.vocab_size,\n",
        "    size=(batch_size, seq_len),\n",
        "    dtype=torch.long,\n",
        "    device=model.device\n",
        ")\n",
        "dummy_attention_mask = torch.ones(\n",
        "    (batch_size, seq_len),\n",
        "    dtype=torch.long,\n",
        "    device=model.device\n",
        ")\n",
        "\n",
        "# export\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    (dummy_input_ids, dummy_attention_mask),\n",
        "    onnx_model_path,\n",
        "    export_params=True,\n",
        "    opset_version=14,\n",
        "    do_constant_folding=True,\n",
        "    input_names=[\"input_ids\", \"attention_mask\"],\n",
        "    output_names=[\"logits\"],\n",
        "    dynamic_axes={\n",
        "        \"input_ids\":       {0: \"batch_size\", 1: \"seq_len\"},\n",
        "        \"attention_mask\":  {0: \"batch_size\", 1: \"seq_len\"},\n",
        "        \"logits\":          {0: \"batch_size\"}\n",
        "    }\n",
        ")\n",
        "\n",
        "# sanity check\n",
        "onnx_model = onnx.load(onnx_model_path)\n",
        "onnx.checker.check_model(onnx_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "hYECqMXnyfdU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYECqMXnyfdU",
        "outputId": "f231350e-c140-446e-b964-da1bdf64e46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size on Disk: 17.61 MB\n"
          ]
        }
      ],
      "source": [
        "model_size = os.path.getsize(onnx_model_path)\n",
        "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2_LFg3-I0pQ2",
      "metadata": {
        "id": "2_LFg3-I0pQ2"
      },
      "source": [
        "### Apply optimizations to ONNX model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark_session(ort_session):\n",
        "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    num_samples = len(test_loader.dataset)\n",
        "    samples_tested = 0\n",
        "\n",
        "    for batch in tqdm(test_loader, desc=\"ONNX Inference\"):\n",
        "        if samples_tested >= num_samples:\n",
        "            break\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].numpy()\n",
        "        attention_mask = batch[\"attention_mask\"].numpy()\n",
        "        labels = batch[\"labels\"].numpy()\n",
        "\n",
        "        outputs = ort_session.run(None, {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask\n",
        "        })[0]\n",
        "\n",
        "        predicted = np.argmax(outputs, axis=1)\n",
        "        batch_size = labels.shape[0]\n",
        "        correct += (predicted == labels).sum()\n",
        "        total += batch_size\n",
        "        samples_tested += batch_size\n",
        "\n",
        "    accuracy = (correct / total) * 100\n",
        "    print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
        "\n",
        "    ## Benchmark inference latency for single sample\n",
        "    num_trials = 100\n",
        "    single_batch = next(iter(test_loader))\n",
        "    input_ids = single_batch[\"input_ids\"][:1].numpy()\n",
        "    attention_mask = single_batch[\"attention_mask\"][:1].numpy()\n",
        "\n",
        "    ort_session.run(None, {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask\n",
        "    })\n",
        "\n",
        "    latencies = []\n",
        "    for _ in range(num_trials):\n",
        "        start = time.time()\n",
        "        ort_session.run(None, {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask\n",
        "        })\n",
        "        latencies.append(time.time() - start)\n",
        "\n",
        "    print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
        "    print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
        "    print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
        "    print(f\"Inference Throughput (single sample): {num_trials / np.sum(latencies):.2f} FPS\")\n",
        "\n",
        "    ## Benchmark batch throughput\n",
        "    num_batches = 50\n",
        "    input_ids = single_batch[\"input_ids\"].numpy()\n",
        "    attention_mask = single_batch[\"attention_mask\"].numpy()\n",
        "\n",
        "    ort_session.run(None, {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask\n",
        "    })\n",
        "\n",
        "    batch_times = []\n",
        "    for _ in range(num_batches):\n",
        "        start = time.time()\n",
        "        ort_session.run(None, {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask\n",
        "        })\n",
        "        batch_times.append(time.time() - start)\n",
        "\n",
        "    batch_fps = (input_ids.shape[0] * num_batches) / np.sum(batch_times)\n",
        "    print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
      ],
      "metadata": {
        "id": "a-y7Fk7cPRJ3"
      },
      "id": "a-y7Fk7cPRJ3",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Base ONNX"
      ],
      "metadata": {
        "id": "PVAT8wCnPnun"
      },
      "id": "PVAT8wCnPnun"
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model_path = \"models/model.onnx\"\n",
        "ort_session = ort.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
        "benchmark_session(ort_session)"
      ],
      "metadata": {
        "id": "2v7f03AbPipO",
        "outputId": "2fd59ab3-94bc-431b-cf00-5f117ad8d9ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2v7f03AbPipO",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution provider: ['CPUExecutionProvider']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ONNX Inference: 100%|██████████| 2821/2821 [04:07<00:00, 11.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 92.02% (332165/360975 correct)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Latency (single sample, median): 1.16 ms\n",
            "Inference Latency (single sample, 95th percentile): 1.59 ms\n",
            "Inference Latency (single sample, 99th percentile): 1.70 ms\n",
            "Inference Throughput (single sample): 779.74 FPS\n",
            "Batch Throughput: 1980.38 FPS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489OE5kZ1khe",
      "metadata": {
        "id": "489OE5kZ1khe"
      },
      "source": [
        "#### Apply basic graph optimizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "sF4gNLhj0nku",
      "metadata": {
        "id": "sF4gNLhj0nku"
      },
      "outputs": [],
      "source": [
        "onnx_model_path = \"models/model.onnx\"\n",
        "optimized_model_path = \"models/model_optimized.onnx\"\n",
        "\n",
        "session_options = ort.SessionOptions()\n",
        "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
        "session_options.optimized_model_filepath = optimized_model_path\n",
        "\n",
        "ort_session = ort.InferenceSession(\n",
        "    onnx_model_path,\n",
        "    sess_options=session_options,\n",
        "    providers=[\"CPUExecutionProvider\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "aHbK_trg1wzC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHbK_trg1wzC",
        "outputId": "64af9da1-4065-4cb3-ce31-ee6819afbbad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution provider: ['CPUExecutionProvider']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ONNX Inference: 100%|██████████| 2821/2821 [04:07<00:00, 11.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 92.02% (332165/360975 correct)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Latency (single sample, median): 1.47 ms\n",
            "Inference Latency (single sample, 95th percentile): 1.54 ms\n",
            "Inference Latency (single sample, 99th percentile): 1.69 ms\n",
            "Inference Throughput (single sample): 680.18 FPS\n",
            "Batch Throughput: 1466.31 FPS\n"
          ]
        }
      ],
      "source": [
        "onnx_model_path = \"models/model_optimized.onnx\"\n",
        "ort_session = ort.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
        "benchmark_session(ort_session)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cIj7mL9J15ry",
      "metadata": {
        "id": "cIj7mL9J15ry"
      },
      "source": [
        "#### Dynamic quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "2aQk5_JD2coU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2aQk5_JD2coU",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "a3a37ff2-a0cf-4bf8-8e1e-faefb009d5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neural-compressor\n",
            "  Downloading neural_compressor-3.3.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting deprecated>=1.2.13 (from neural-compressor)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting numpy<2.0 (from neural-compressor)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from neural-compressor) (4.11.0.86)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from neural-compressor) (2.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from neural-compressor) (11.2.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from neural-compressor) (3.16.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from neural-compressor) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from neural-compressor) (9.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from neural-compressor) (2.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from neural-compressor) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from neural-compressor) (2.32.3)\n",
            "Collecting schema (from neural-compressor)\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from neural-compressor) (1.6.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.13->neural-compressor) (1.17.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->neural-compressor) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->neural-compressor) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->neural-compressor) (2025.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable->neural-compressor) (0.2.13)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools->neural-compressor) (3.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->neural-compressor) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->neural-compressor) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->neural-compressor) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->neural-compressor) (2025.4.26)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->neural-compressor) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->neural-compressor) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->neural-compressor) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->neural-compressor) (1.17.0)\n",
            "Downloading neural_compressor-3.3.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: schema, numpy, deprecated, neural-compressor\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deprecated-1.2.18 neural-compressor-3.3.1 numpy-1.26.4 schema-0.7.7\n"
          ]
        }
      ],
      "source": [
        "!pip install neural-compressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "IfBccYP017gd",
      "metadata": {
        "id": "IfBccYP017gd"
      },
      "outputs": [],
      "source": [
        "import neural_compressor\n",
        "from neural_compressor import quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "tUCYjwEW2Mdv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUCYjwEW2Mdv",
        "outputId": "79b81bca-40ce-4b06-b066-8a4964a54fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-16 22:47:35 [INFO] Start auto tuning.\n",
            "2025-05-16 22:47:35 [INFO] Quantize model without tuning!\n",
            "2025-05-16 22:47:35 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
            "2025-05-16 22:47:35 [INFO] Adaptor has 5 recipes.\n",
            "2025-05-16 22:47:35 [INFO] 0 recipes specified by user.\n",
            "2025-05-16 22:47:35 [INFO] 3 recipes require future tuning.\n",
            "2025-05-16 22:47:35 [INFO] *** Initialize auto tuning\n",
            "2025-05-16 22:47:35 [INFO] {\n",
            "2025-05-16 22:47:35 [INFO]     'PostTrainingQuantConfig': {\n",
            "2025-05-16 22:47:35 [INFO]         'AccuracyCriterion': {\n",
            "2025-05-16 22:47:35 [INFO]             'criterion': 'relative',\n",
            "2025-05-16 22:47:35 [INFO]             'higher_is_better': True,\n",
            "2025-05-16 22:47:35 [INFO]             'tolerable_loss': 0.01,\n",
            "2025-05-16 22:47:35 [INFO]             'absolute': None,\n",
            "2025-05-16 22:47:35 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x798d5433d450>>,\n",
            "2025-05-16 22:47:35 [INFO]             'relative': 0.01\n",
            "2025-05-16 22:47:35 [INFO]         },\n",
            "2025-05-16 22:47:35 [INFO]         'approach': 'post_training_dynamic_quant',\n",
            "2025-05-16 22:47:35 [INFO]         'backend': 'default',\n",
            "2025-05-16 22:47:35 [INFO]         'calibration_sampling_size': [\n",
            "2025-05-16 22:47:35 [INFO]             100\n",
            "2025-05-16 22:47:35 [INFO]         ],\n",
            "2025-05-16 22:47:35 [INFO]         'device': 'cpu',\n",
            "2025-05-16 22:47:35 [INFO]         'domain': 'auto',\n",
            "2025-05-16 22:47:35 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
            "2025-05-16 22:47:35 [INFO]         'excluded_precisions': [\n",
            "2025-05-16 22:47:35 [INFO]         ],\n",
            "2025-05-16 22:47:35 [INFO]         'framework': 'onnxruntime',\n",
            "2025-05-16 22:47:35 [INFO]         'inputs': [\n",
            "2025-05-16 22:47:35 [INFO]         ],\n",
            "2025-05-16 22:47:35 [INFO]         'model_name': '',\n",
            "2025-05-16 22:47:35 [INFO]         'op_name_dict': None,\n",
            "2025-05-16 22:47:35 [INFO]         'op_type_dict': None,\n",
            "2025-05-16 22:47:35 [INFO]         'outputs': [\n",
            "2025-05-16 22:47:35 [INFO]         ],\n",
            "2025-05-16 22:47:35 [INFO]         'quant_format': 'default',\n",
            "2025-05-16 22:47:35 [INFO]         'quant_level': 'auto',\n",
            "2025-05-16 22:47:35 [INFO]         'recipes': {\n",
            "2025-05-16 22:47:35 [INFO]             'smooth_quant': False,\n",
            "2025-05-16 22:47:35 [INFO]             'smooth_quant_args': {\n",
            "2025-05-16 22:47:35 [INFO]             },\n",
            "2025-05-16 22:47:35 [INFO]             'layer_wise_quant': False,\n",
            "2025-05-16 22:47:35 [INFO]             'layer_wise_quant_args': {\n",
            "2025-05-16 22:47:35 [INFO]             },\n",
            "2025-05-16 22:47:35 [INFO]             'fast_bias_correction': False,\n",
            "2025-05-16 22:47:35 [INFO]             'weight_correction': False,\n",
            "2025-05-16 22:47:35 [INFO]             'gemm_to_matmul': True,\n",
            "2025-05-16 22:47:35 [INFO]             'graph_optimization_level': None,\n",
            "2025-05-16 22:47:35 [INFO]             'first_conv_or_matmul_quantization': True,\n",
            "2025-05-16 22:47:35 [INFO]             'last_conv_or_matmul_quantization': True,\n",
            "2025-05-16 22:47:35 [INFO]             'pre_post_process_quantization': True,\n",
            "2025-05-16 22:47:35 [INFO]             'add_qdq_pair_to_weight': False,\n",
            "2025-05-16 22:47:35 [INFO]             'optypes_to_exclude_output_quant': [\n",
            "2025-05-16 22:47:35 [INFO]             ],\n",
            "2025-05-16 22:47:35 [INFO]             'dedicated_qdq_pair': False,\n",
            "2025-05-16 22:47:35 [INFO]             'rtn_args': {\n",
            "2025-05-16 22:47:35 [INFO]             },\n",
            "2025-05-16 22:47:35 [INFO]             'awq_args': {\n",
            "2025-05-16 22:47:35 [INFO]             },\n",
            "2025-05-16 22:47:35 [INFO]             'gptq_args': {\n",
            "2025-05-16 22:47:35 [INFO]             },\n",
            "2025-05-16 22:47:35 [INFO]             'teq_args': {\n",
            "2025-05-16 22:47:35 [INFO]             },\n",
            "2025-05-16 22:47:35 [INFO]             'autoround_args': {\n",
            "2025-05-16 22:47:35 [INFO]             }\n",
            "2025-05-16 22:47:35 [INFO]         },\n",
            "2025-05-16 22:47:35 [INFO]         'reduce_range': None,\n",
            "2025-05-16 22:47:35 [INFO]         'TuningCriterion': {\n",
            "2025-05-16 22:47:35 [INFO]             'max_trials': 100,\n",
            "2025-05-16 22:47:35 [INFO]             'objective': [\n",
            "2025-05-16 22:47:35 [INFO]                 'performance'\n",
            "2025-05-16 22:47:35 [INFO]             ],\n",
            "2025-05-16 22:47:35 [INFO]             'strategy': 'basic',\n",
            "2025-05-16 22:47:35 [INFO]             'strategy_kwargs': None,\n",
            "2025-05-16 22:47:35 [INFO]             'timeout': 0\n",
            "2025-05-16 22:47:35 [INFO]         },\n",
            "2025-05-16 22:47:35 [INFO]         'use_bf16': True,\n",
            "2025-05-16 22:47:35 [INFO]         'ni_workload_name': 'quantization'\n",
            "2025-05-16 22:47:35 [INFO]     }\n",
            "2025-05-16 22:47:35 [INFO] }\n",
            "2025-05-16 22:47:35 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
            "2025-05-16 22:47:35 [WARNING] The model is automatically detected as an NLP model. You can use 'domain' argument in 'PostTrainingQuantConfig' to overwrite it\n",
            "2025-05-16 22:47:35 [WARNING] Graph optimization level is automatically set to ENABLE_EXTENDED. You can use 'recipe' argument in 'PostTrainingQuantConfig'to overwrite it\n",
            "2025-05-16 22:47:35 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
            "2025-05-16 22:47:35 [INFO] Quantize the model with default config.\n",
            "2025-05-16 22:47:36 [INFO] |**********Mixed Precision Statistics*********|\n",
            "2025-05-16 22:47:36 [INFO] +-----------------------+-------+------+------+\n",
            "2025-05-16 22:47:36 [INFO] |        Op Type        | Total | INT8 | FP32 |\n",
            "2025-05-16 22:47:36 [INFO] +-----------------------+-------+------+------+\n",
            "2025-05-16 22:47:36 [INFO] |         MatMul        |   17  |  13  |  4   |\n",
            "2025-05-16 22:47:36 [INFO] |         Gather        |   14  |  3   |  11  |\n",
            "2025-05-16 22:47:36 [INFO] |    DequantizeLinear   |   3   |  3   |  0   |\n",
            "2025-05-16 22:47:36 [INFO] | DynamicQuantizeLinear |   9   |  9   |  0   |\n",
            "2025-05-16 22:47:36 [INFO] +-----------------------+-------+------+------+\n",
            "2025-05-16 22:47:36 [INFO] Pass quantize model elapsed time: 576.55 ms\n",
            "2025-05-16 22:47:36 [INFO] Save tuning history to /content/nc_workspace/2025-05-16_22-45-51/./history.snapshot.\n",
            "2025-05-16 22:47:36 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
            "2025-05-16 22:47:36 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
            "2025-05-16 22:47:36 [INFO] Save deploy yaml to /content/nc_workspace/2025-05-16_22-45-51/deploy.yaml\n"
          ]
        }
      ],
      "source": [
        "# Load ONNX model\n",
        "model_path = \"models/model.onnx\"\n",
        "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
        "\n",
        "# Configure dynamic quantization\n",
        "config_ptq = neural_compressor.PostTrainingQuantConfig(\n",
        "    approach=\"dynamic\"\n",
        ")\n",
        "\n",
        "# Quantize\n",
        "q_model = quantization.fit(\n",
        "    model=fp32_model,\n",
        "    conf=config_ptq\n",
        ")\n",
        "\n",
        "# Save quantized model\n",
        "quant_model_path = \"models/model_quantized_dynamic.onnx\"\n",
        "q_model.save_model_to_file(quant_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "TUmNu4CG2Ye8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUmNu4CG2Ye8",
        "outputId": "69c72a81-d1a5-4caa-aa98-2635833d16bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size on Disk: 4.51 MB\n"
          ]
        }
      ],
      "source": [
        "# Print model size\n",
        "model_size = os.path.getsize(quant_model_path)\n",
        "print(f\"Model Size on Disk: {model_size / 1e6:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "lu2XCw5a2Zic",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu2XCw5a2Zic",
        "outputId": "3230dc0c-110f-4fde-cb7e-731ff3566e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution provider: ['CPUExecutionProvider']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ONNX Inference: 100%|██████████| 2821/2821 [03:34<00:00, 13.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 92.02% (332165/360975 correct)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Latency (single sample, median): 1.52 ms\n",
            "Inference Latency (single sample, 95th percentile): 1.62 ms\n",
            "Inference Latency (single sample, 99th percentile): 1.67 ms\n",
            "Inference Throughput (single sample): 653.39 FPS\n",
            "Batch Throughput: 1767.16 FPS\n"
          ]
        }
      ],
      "source": [
        "ort_session = ort.InferenceSession(quant_model_path, providers=[\"CPUExecutionProvider\"])\n",
        "benchmark_session(ort_session)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oWmA1Xxl3t8d",
      "metadata": {
        "id": "oWmA1Xxl3t8d"
      },
      "source": [
        "### CUDA execution provider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "XARdH3BJ3mLh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "XARdH3BJ3mLh",
        "outputId": "95cb63f5-c1a1-470f-e2b5-94cac8c53957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution provider: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ONNX Inference: 100%|██████████| 2821/2821 [00:36<00:00, 78.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 92.02% (332165/360975 correct)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Latency (single sample, median): 0.72 ms\n",
            "Inference Latency (single sample, 95th percentile): 0.79 ms\n",
            "Inference Latency (single sample, 99th percentile): 0.87 ms\n",
            "Inference Throughput (single sample): 1361.41 FPS\n",
            "Batch Throughput: 78767.20 FPS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GPU'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "onnx_model_path = \"models/model.onnx\"\n",
        "ort_session = ort.InferenceSession(onnx_model_path, providers=[\"CUDAExecutionProvider\"])\n",
        "benchmark_session(ort_session)\n",
        "ort.get_device()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}